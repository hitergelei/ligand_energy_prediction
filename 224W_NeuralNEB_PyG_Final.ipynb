{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBpRldjz97q3"
      },
      "source": [
        "# Investigating Ligand-Protein Docking with Graph Neural Networks\n",
        "*This work was done by Junha Lee for the Fall 2023 CS224W Final Project.*\n",
        "\n",
        "See the writeup in this Medium blog post: https://medium.com/@junhakunha/cs-224w-project-e433ae0e7ce8\n",
        "\n",
        "This colab trains a polarizable atom interaction neural network (PaiNN) model on the QM9x/Transition1x dataset, for use in predicting the energies of 3D molecular conformations. The model is then used as a pseudopotential for the NEB method, to generate reaction pathways of ligands bound to proteins.\n",
        "\n",
        "NeuralNEB, QM9x Dataset: https://arxiv.org/abs/2207.09971  \n",
        "Transition1x Dataset: https://pubmed.ncbi.nlm.nih.gov/36566281/  \n",
        "Nudged Elastic Band (NEB) method: https://www.worldscientific.com/doi/abs/10.1142/9789812839664_0016  \n",
        "\n",
        "The following code combines elements of the following codebases.\n",
        "\n",
        "NeuralNEB: https://gitlab.com/matschreiner/neuralneb  \n",
        "Transition1x: https://gitlab.com/matschreiner/Transition1x  \n",
        "PaiNN-in-PyG: https://github.com/MaxH1996/PaiNN-in-PyG  \n",
        "\n",
        "Significant parts of each code were modified to be used for this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adV2vwq3EkgS"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DtSWCIwcC0P"
      },
      "source": [
        "## Install PyG (``pytorch_geomtric``)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-ZXh6_GwQyU"
      },
      "source": [
        "Pytorch Geometric: https://pytorch-geometric.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLQaCU-0cJKQ",
        "outputId": "9a457126-d623-4981-b3f8-a33fe2977249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdTCK4BfcKXI"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import MessagePassing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzxNZPVfOBBR"
      },
      "source": [
        "## Import standard libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzFo7MIPNCtA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import itertools\n",
        "import h5py\n",
        "import progressbar\n",
        "import json\n",
        "from urllib.request import urlretrieve\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, List\n",
        "\n",
        "import scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUkxY4H2OFTG"
      },
      "source": [
        "## Install ``ase``\n",
        "\n",
        "Atomic Simulation Environment package: https://wiki.fysik.dtu.dk/ase/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGFh37cUN9PK",
        "outputId": "2beb2be3-3533-45da-c458-fdfdc95ea64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ase\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from ase) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from ase) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase) (1.16.0)\n",
            "Installing collected packages: ase\n",
            "Successfully installed ase-3.22.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oqhdzmsEno6"
      },
      "outputs": [],
      "source": [
        "import ase.io\n",
        "import ase.db\n",
        "\n",
        "from ase import Atoms\n",
        "from ase.calculators.calculator import Calculator, all_changes\n",
        "\n",
        "from ase.io import read, write\n",
        "from ase.neb import NEB, NEBOptimizer, NEBTools\n",
        "from ase.optimize.bfgs import BFGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol05g-tFaLjV"
      },
      "source": [
        "## Other Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fom_NOkzfVFz"
      },
      "outputs": [],
      "source": [
        "!mkdir neuralneb\n",
        "!mkdir neuralneb/data\n",
        "!mkdir neuralneb/models\n",
        "!mkdir neuralneb/results\n",
        "!mkdir neuralneb/test_reaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfNvRskIL37E"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KfffhoKQki7"
      },
      "outputs": [],
      "source": [
        "class ProgressBar:\n",
        "    def __init__(self):\n",
        "        self.pbar = None\n",
        "\n",
        "    def __call__(self, block_num, block_size, total_size):\n",
        "        if not self.pbar:\n",
        "            self.pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "            self.pbar.start()\n",
        "\n",
        "        downloaded = block_num * block_size\n",
        "        if downloaded < total_size:\n",
        "            self.pbar.update(downloaded)\n",
        "        else:\n",
        "            self.pbar.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zroi_W93QMDQ"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vxVGIWEX5Ed"
      },
      "source": [
        "## QM9x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tokhsac3TB0G"
      },
      "outputs": [],
      "source": [
        "def download_qm9x(dir):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "    path = os.path.join(dir, \"qm9x.db\")\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        print(f\"QM9x data already exists\")\n",
        "    else:\n",
        "        print(f\"Downloading QM9x data to {dir}/qm9x.db\")\n",
        "        urlretrieve(\n",
        "            \"https://figshare.com/ndownloader/files/36693216\",\n",
        "            path,\n",
        "            ProgressBar(),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WzxJiKaX8E0"
      },
      "source": [
        "## Transition1x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHIAecTjTKRq"
      },
      "outputs": [],
      "source": [
        "def download_transition1x(dir):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "    path = os.path.join(dir, \"transition1x.h5\")\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        print(f\"Transition1x data already exists\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Downloading Transition1x data to {dir}/transition1x.h5\")\n",
        "        urlretrieve(\n",
        "            \"https://figshare.com/ndownloader/files/36035789\",\n",
        "            path,\n",
        "            ProgressBar()\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXtR9En-Zgsf"
      },
      "outputs": [],
      "source": [
        "REFERENCE_ENERGIES = {\n",
        "    1: -13.62222753701504,\n",
        "    6: -1029.4130839658328,\n",
        "    7: -1484.8710358098756,\n",
        "    8: -2041.8396277138045,\n",
        "    9: -2712.8213146878606,\n",
        "}\n",
        "\n",
        "\n",
        "def get_molecular_reference_energy(atomic_numbers):\n",
        "    molecular_reference_energy = 0\n",
        "    for atomic_number in atomic_numbers:\n",
        "        molecular_reference_energy += REFERENCE_ENERGIES[atomic_number]\n",
        "\n",
        "    return molecular_reference_energy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1Ww3KhKZi-G"
      },
      "outputs": [],
      "source": [
        "def generator(formula, rxn, grp):\n",
        "    \"\"\" Iterates through a h5 group \"\"\"\n",
        "\n",
        "    energies = grp[\"wB97x_6-31G(d).energy\"]\n",
        "    forces = grp[\"wB97x_6-31G(d).forces\"]\n",
        "    atomic_numbers = list(grp[\"atomic_numbers\"])\n",
        "    positions = grp[\"positions\"]\n",
        "    molecular_reference_energy = get_molecular_reference_energy(atomic_numbers)\n",
        "\n",
        "    for energy, force, positions in zip(energies, forces, positions):\n",
        "        d = {\n",
        "            \"rxn\": rxn,\n",
        "            \"wB97x_6-31G(d).energy\": energy.__float__(),\n",
        "            \"wB97x_6-31G(d).atomization_energy\": energy\n",
        "            - molecular_reference_energy.__float__(),\n",
        "            \"wB97x_6-31G(d).forces\": force.tolist(),\n",
        "            \"positions\": positions,\n",
        "            \"formula\": formula,\n",
        "            \"atomic_numbers\": atomic_numbers,\n",
        "        }\n",
        "\n",
        "        yield d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQUwjaaIUApz"
      },
      "outputs": [],
      "source": [
        "class Dataloader_t1x:\n",
        "    \"\"\"\n",
        "    Can iterate through h5 data set for paper ####\n",
        "\n",
        "    hdf5_file: path to data\n",
        "    only_final: if True, the iterator will only loop through reactant, product and transition\n",
        "    state instead of all configurations for each reaction and return them in dictionaries.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_file, datasplit=\"data\", only_final=False):\n",
        "        self.hdf5_file = hdf5_file\n",
        "        self.only_final = only_final\n",
        "\n",
        "        self.datasplit = datasplit\n",
        "        if datasplit:\n",
        "            assert datasplit in [\n",
        "                \"data\",\n",
        "                \"train\",\n",
        "                \"val\",\n",
        "                \"test\",\n",
        "            ], \"datasplit must be one of 'all', 'train', 'val' or 'test'\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        with h5py.File(self.hdf5_file, \"r\") as f:\n",
        "            split = f[self.datasplit]\n",
        "\n",
        "            for formula, grp in split.items():\n",
        "                for rxn, subgrp in grp.items():\n",
        "                    reactant = next(generator(formula, rxn, subgrp[\"reactant\"]))\n",
        "                    product = next(generator(formula, rxn, subgrp[\"product\"]))\n",
        "\n",
        "                    if self.only_final:\n",
        "                        transition_state = next(\n",
        "                            generator(formula, rxn, subgrp[\"transition_state\"])\n",
        "                        )\n",
        "                        yield {\n",
        "                            \"rxn\": rxn,\n",
        "                            \"reactant\": reactant,\n",
        "                            \"product\": product,\n",
        "                            \"transition_state\": transition_state,\n",
        "                        }\n",
        "                    else:\n",
        "                        yield reactant\n",
        "                        yield product\n",
        "                        for molecule in generator(formula, rxn, subgrp):\n",
        "                            yield molecule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6JrXFoITys0"
      },
      "outputs": [],
      "source": [
        "def generateTransition1xDB(dir, download_trainset=True):\n",
        "    h5file = os.path.join(dir, \"transition1x.h5\")\n",
        "    train_db = os.path.join(dir, \"transition1x_train.db\")\n",
        "    test_db = os.path.join(dir, \"transition1x_test.db\")\n",
        "    val_db = os.path.join(dir, \"transition1x_val.db\")\n",
        "\n",
        "    if os.path.exists(train_db):\n",
        "        print(f\"File {train_db} db already exists\")\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "            print(f\"Downloading Transition1x splits to {dir}\")\n",
        "            if download_trainset:\n",
        "                urlretrieve(\n",
        "                    \"https://figshare.com/ndownloader/files/43605210\",\n",
        "                    train_db,\n",
        "                    ProgressBar()\n",
        "                )\n",
        "            urlretrieve(\n",
        "                \"https://figshare.com/ndownloader/files/43605861\",\n",
        "                test_db,\n",
        "                ProgressBar()\n",
        "            )\n",
        "            urlretrieve(\n",
        "                \"https://figshare.com/ndownloader/files/43605864\",\n",
        "                val_db,\n",
        "                ProgressBar()\n",
        "            )\n",
        "        except: # Convert manually\n",
        "            print(f\"download failed, manually converting {h5file}\")\n",
        "\n",
        "            dataloaders = {\n",
        "                \"train\": Dataloader_t1x(h5file, \"train\"),\n",
        "                \"test\": Dataloader_t1x(h5file, \"test\"),\n",
        "                \"val\": Dataloader_t1x(h5file, \"val\"),\n",
        "            }\n",
        "\n",
        "\n",
        "            for split, dataloader in dataloaders.items():\n",
        "                with ase.db.connect(f\"data/transition1x_{split}.db\") as db:\n",
        "                    for configuration in tqdm(dataloader):\n",
        "                        atoms = Atoms(configuration[\"atomic_numbers\"])\n",
        "                        atoms.set_positions(configuration[\"positions\"])\n",
        "\n",
        "                        data = {\n",
        "                            \"energy\": configuration[\"wB97x_6-31G(d).atomization_energy\"],\n",
        "                            \"forces\": configuration[\"wB97x_6-31G(d).forces\"],\n",
        "                        }\n",
        "                        idx = db.write(atoms, data=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2mAC7S7ZvOm"
      },
      "source": [
        "## Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0kFUzbHQQgt",
        "outputId": "25173ad9-4c55-48a4-8314-08523e667795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QM9x data already exists\n",
            "Transition1x data already exists\n",
            "Downloading Transition1x splits to neuralneb/data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100% (19235934208 of 19235934208) |######| Elapsed Time: 0:19:39 Time:  0:19:39\n",
            "100% (463314944 of 463314944) |##########| Elapsed Time: 0:00:28 Time:  0:00:28\n",
            "100% (538488832 of 538488832) |##########| Elapsed Time: 0:00:32 Time:  0:00:32\n"
          ]
        }
      ],
      "source": [
        "use_qm9x = True\n",
        "use_t1x = True\n",
        "\n",
        "dir = \"neuralneb/data\"\n",
        "\n",
        "if use_qm9x:\n",
        "    download_qm9x(dir)\n",
        "if use_t1x:\n",
        "    download_transition1x(dir)\n",
        "    generateTransition1xDB(dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2qZuAE0NmTt"
      },
      "source": [
        "# PaiNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeAUEBnUE95d"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MYha-CEJNUVL"
      },
      "outputs": [],
      "source": [
        "def shifted_softplus(x):\n",
        "    \"\"\"\n",
        "    Compute shifted soft-plus activation function.\n",
        "    .. math::\n",
        "       y = \\ln\\left(1 + e^{-x}\\right) - \\ln(2)\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): input tensor.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: shifted soft-plus of input.\n",
        "\n",
        "    \"\"\"\n",
        "    return nn.functional.softplus(x) - np.log(2.0)\n",
        "\n",
        "\n",
        "class ShiftedSoftplus(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return shifted_softplus(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SPGmlGS4NXmj"
      },
      "outputs": [],
      "source": [
        "def unpad_and_cat(stacked_seq: torch.Tensor, seq_len: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Unpad and concatenate by removing batch dimension\n",
        "\n",
        "    Args:\n",
        "        stacked_seq: (batch_size, max_length, *) Tensor\n",
        "        seq_len: (batch_size) Tensor with length of each sequence\n",
        "\n",
        "    Returns:\n",
        "        (prod(seq_len), *) Tensor\n",
        "\n",
        "    \"\"\"\n",
        "    unstacked = stacked_seq.unbind(0)\n",
        "    unpadded = [\n",
        "        torch.narrow(t, 0, 0, l) for (t, l) in zip(unstacked, seq_len.unbind(0))\n",
        "    ]\n",
        "    return torch.cat(unpadded, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "im1bvVPsNcoE"
      },
      "outputs": [],
      "source": [
        "def pad_and_stack(tensors: List[torch.Tensor]):\n",
        "    \"\"\"Pad list of tensors if tensors are arrays and stack if they are scalars\"\"\"\n",
        "    if tensors[0].shape:\n",
        "        return torch.nn.utils.rnn.pad_sequence(\n",
        "            tensors, batch_first=True, padding_value=0\n",
        "        )\n",
        "    return torch.stack(tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jOthYAFuNeaN"
      },
      "outputs": [],
      "source": [
        "def sum_splits(values: torch.Tensor, splits: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Sum across dimension 0 of the tensor `values` in chunks\n",
        "    defined in `splits`\n",
        "\n",
        "    Args:\n",
        "        values: Tensor of shape (`prod(splits)`, *)\n",
        "        splits: 1-dimensional tensor with size of each chunk\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape (`splits.shape[0]`, *)\n",
        "\n",
        "    \"\"\"\n",
        "    # prepare an index vector for summation\n",
        "    ind = torch.zeros(splits.sum(), dtype=splits.dtype, device=splits.device)\n",
        "    ind[torch.cumsum(splits, dim=0)[:-1]] = 1\n",
        "    ind = torch.cumsum(ind, dim=0)\n",
        "    # prepare the output\n",
        "    sum_y = torch.zeros(\n",
        "        splits.shape + values.shape[1:], dtype=values.dtype, device=values.device\n",
        "    )\n",
        "    # do the actual summation\n",
        "    sum_y.index_add_(0, ind, values)\n",
        "    return sum_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z4NAneScNghp"
      },
      "outputs": [],
      "source": [
        "def calc_distance(\n",
        "    positions: torch.Tensor,\n",
        "    cells: torch.Tensor,\n",
        "    edges: torch.Tensor,\n",
        "    edges_displacement: torch.Tensor,\n",
        "    splits: torch.Tensor,\n",
        "    return_diff=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate distance of edges\n",
        "\n",
        "    Args:\n",
        "        positions: Tensor of shape (num_nodes, 3) with xyz coordinates inside cell\n",
        "        cells: Tensor of shape (num_splits, 3, 3) with one unit cell for each split\n",
        "        edges: Tensor of shape (num_edges, 2)\n",
        "        edges_displacement: Tensor of shape (num_edges, 3) with the offset (in number of cell vectors) of the sending node\n",
        "        splits: 1-dimensional tensor with the number of edges for each separate graph\n",
        "        return_diff: If non-zero return the also the vector corresponding to edges\n",
        "    \"\"\"\n",
        "    unitcell_repeat = torch.repeat_interleave(cells, splits, dim=0)  # num_edges, 3, 3\n",
        "    displacement = torch.matmul(\n",
        "        torch.unsqueeze(edges_displacement, 1), unitcell_repeat\n",
        "    )  # num_edges, 1, 3\n",
        "    displacement = torch.squeeze(displacement, dim=1)\n",
        "    neigh_pos = positions[edges[:, 0]]  # num_edges, 3\n",
        "    neigh_abs_pos = neigh_pos + displacement  # num_edges, 3\n",
        "    this_pos = positions[edges[:, 1]]  # num_edges, 3\n",
        "    diff = this_pos - neigh_abs_pos  # num_edges, 3\n",
        "    dist = torch.sqrt(\n",
        "        torch.sum(torch.square(diff), dim=1, keepdim=True)\n",
        "    )  # num_edges, 1\n",
        "\n",
        "    if return_diff:\n",
        "        return dist, diff\n",
        "    else:\n",
        "        return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "garr3ArQzFvQ"
      },
      "outputs": [],
      "source": [
        "class CosineCutoff(torch.nn.Module):\n",
        "    def __init__(self, cutoff=5.0):\n",
        "        super(CosineCutoff, self).__init__()\n",
        "        # self.register_buffer(\"cutoff\", torch.FloatTensor([cutoff]))\n",
        "        self.cutoff = cutoff\n",
        "\n",
        "    def forward(self, distances):\n",
        "        \"\"\"Compute cutoff.\n",
        "\n",
        "        Args:\n",
        "            distances (torch.Tensor): values of interatomic distances.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: values of cutoff function.\n",
        "\n",
        "        \"\"\"\n",
        "        # Compute values of cutoff function\n",
        "        cutoffs = 0.5 * (torch.cos(distances * np.pi / self.cutoff) + 1.0)\n",
        "        # Remove contributions beyond the cutoff radius\n",
        "        cutoffs *= (distances < self.cutoff).float()\n",
        "        return cutoffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZuuHEm1UrQW9"
      },
      "outputs": [],
      "source": [
        "class BesselBasis(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Sine for radial basis expansion with coulomb decay. (0th order Bessel from DimeNet)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cutoff=5.0, n_rbf=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            cutoff: radial cutoff\n",
        "            n_rbf: number of basis functions.\n",
        "        \"\"\"\n",
        "        super(BesselBasis, self).__init__()\n",
        "        # compute offset and width of Gaussian functions\n",
        "        freqs = torch.arange(1, n_rbf + 1) * math.pi / cutoff\n",
        "        self.register_buffer(\"freqs\", freqs)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = torch.norm(inputs, p=2, dim=1)\n",
        "        a = self.freqs\n",
        "        ax = torch.outer(inputs, a)\n",
        "        sinax = torch.sin(ax)\n",
        "\n",
        "        norm = torch.where(inputs == 0, torch.tensor(1.0, device=inputs.device), inputs)\n",
        "        y = sinax / norm[:, None]\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Zslx89Ogp7"
      },
      "source": [
        "## PaiNN Message Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0tF8fBfJOf1n"
      },
      "outputs": [],
      "source": [
        "class PaiNNMessage(MessagePassing):\n",
        "    \"\"\"Interaction network\"\"\"\n",
        "\n",
        "    def __init__(self, node_size, cutoff=5.0, n_rbf=20):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            node_size (int): Size of node state\n",
        "            cutoff (float): Cutoff distance\n",
        "            n_rbf (int): Number of basis functions for RBF layer\n",
        "        \"\"\"\n",
        "        # Use sum aggregation for messages\n",
        "        super(PaiNNMessage, self).__init__(aggr=\"add\")\n",
        "\n",
        "        self.node_size = node_size\n",
        "        self.scalar_message_mlp = nn.Sequential(\n",
        "            nn.Linear(node_size, node_size),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(node_size, 3 * node_size),\n",
        "        )\n",
        "        self.RBF = BesselBasis(cutoff, n_rbf)\n",
        "        self.lin_rbf = nn.Linear(n_rbf, 3 * node_size)\n",
        "        self.f_cut = CosineCutoff(cutoff)\n",
        "\n",
        "    def forward(self, node_state_scalar, node_state_vector, edge_vector, edges):\n",
        "        # Flatten s,v and concatenate to form node feature x for message passing.\n",
        "        # s,v will be restored to their original shapes when used.\n",
        "        s = node_state_scalar.flatten(-1)\n",
        "        v = node_state_vector.flatten(-2)\n",
        "        flat_shape_v = v.shape[-1]\n",
        "        flat_shape_s = s.shape[-1]\n",
        "        x = torch.cat([s, v], dim=-1)\n",
        "\n",
        "        # Propagate messages\n",
        "        x = self.propagate(edges,\n",
        "            x=x,\n",
        "            edge_attr=edge_vector,\n",
        "            flat_shape_s=flat_shape_s,\n",
        "            flat_shape_v=flat_shape_v,\n",
        "        )\n",
        "\n",
        "        return x\n",
        "\n",
        "    def message(self, x_j, edge_attr, flat_shape_s, flat_shape_v):\n",
        "        # Restore s_j and v_j from x_j\n",
        "        s_j, v_j = torch.split(x_j, [flat_shape_s, flat_shape_v], dim=-1)\n",
        "        v_j = v_j.reshape(-1, int(flat_shape_v / 3), 3)\n",
        "\n",
        "        # r_ij left channel\n",
        "        rbf = self.RBF(edge_attr)\n",
        "        ch1 = self.lin_rbf(rbf)\n",
        "        cut = self.f_cut(edge_attr.norm(dim=-1))\n",
        "        W = torch.einsum(\"ij,i->ij\", ch1, cut)\n",
        "\n",
        "        # r_ij right channel\n",
        "        normalized = F.normalize(edge_attr, p=2, dim=1)\n",
        "\n",
        "        # s_j channel\n",
        "        phi = self.scalar_message_mlp(s_j)\n",
        "        left, dsm, right = torch.split(phi * W, self.node_size, dim=-1)\n",
        "\n",
        "        # v_j channel\n",
        "        hadamard_right = torch.einsum(\"ij,ik->ijk\", right, normalized)\n",
        "        hadamard_left = torch.einsum(\"ijk,ij->ijk\", v_j, left)\n",
        "        dvm = hadamard_left + hadamard_right\n",
        "\n",
        "        # Prepare vector for update\n",
        "        # (note that this is the residual to be added to previous layer)\n",
        "        x_j = torch.cat((dsm, dvm.flatten(-2)), dim=-1)\n",
        "\n",
        "        return x_j\n",
        "\n",
        "\n",
        "    def update(self, out_aggr, flat_shape_s, flat_shape_v):\n",
        "        # Recover residuals\n",
        "        delta_s, delta_v = torch.split(out_aggr, [flat_shape_s, flat_shape_v], dim=-1)\n",
        "        delta_v = torch.transpose(delta_v.reshape(-1, int(flat_shape_v / 3), 3), 1, 2)\n",
        "\n",
        "        return delta_s, delta_v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg_v7LtWOllo"
      },
      "source": [
        "## PaiNN Update Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rel97xsMEuu5"
      },
      "outputs": [],
      "source": [
        "class PaiNNUpdate(nn.Module):\n",
        "    \"\"\"PaiNN style update network. Models the interaction between scalar and vectorial part\"\"\"\n",
        "\n",
        "    def __init__(self, node_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linearU = nn.Linear(node_size, node_size, bias=False)\n",
        "        self.linearV = nn.Linear(node_size, node_size, bias=False)\n",
        "        self.combined_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * node_size, node_size),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(node_size, 3 * node_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, node_state_scalar, node_state_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            node_state_scalar (tensor): Node states (num_nodes, node_size)\n",
        "            node_state_vector (tensor): Node states (num_nodes, 3, node_size)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of 2 tensors:\n",
        "                updated_node_state_scalar (num_nodes, node_size)\n",
        "                updated_node_state_vector (num_nodes, 3, node_size)\n",
        "        \"\"\"\n",
        "\n",
        "        Uv = self.linearU(node_state_vector)  # num_nodes, 3, node_size\n",
        "        Vv = self.linearV(node_state_vector)  # num_nodes, 3, node_size\n",
        "\n",
        "        Vv_norm_squared = torch.sum(\n",
        "            torch.square(Vv), dim=1, keepdim=False\n",
        "        )\n",
        "\n",
        "\n",
        "        mlp_input = torch.cat(\n",
        "            (node_state_scalar, Vv_norm_squared), dim=1\n",
        "        )  # num_nodes, node_size*2\n",
        "        mlp_output = self.combined_mlp(mlp_input)\n",
        "\n",
        "        a_ss, a_sv, a_vv = torch.split(\n",
        "            mlp_output, node_state_scalar.shape[1], dim=1\n",
        "        )  # num_nodes, node_size\n",
        "\n",
        "        inner_prod = torch.sum(Uv * Vv, dim=1)  # num_nodes, node_size\n",
        "\n",
        "        delta_v = torch.unsqueeze(a_vv, 1) * Uv  # num_nodes, 3, node_size\n",
        "\n",
        "        delta_s = a_ss + a_sv * inner_prod  # num_nodes, node_size\n",
        "\n",
        "        return delta_s, delta_v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-dKzK-tO43B"
      },
      "source": [
        "## PaiNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HRSOj7DGO3J3"
      },
      "outputs": [],
      "source": [
        "class PaiNN(nn.Module):\n",
        "    \"\"\"PainnModel with forces.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_interactions,\n",
        "        hidden_state_size,\n",
        "        cutoff=5.0,\n",
        "        n_rbf=20,\n",
        "        target_mean=None,\n",
        "        target_stddev=None,\n",
        "        normalize_atomwise=True,\n",
        "        direct_force_output=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_interactions (int): Number of interaction layers\n",
        "            hidden_state_size (int): Size of hidden node states\n",
        "            cutoff (float): Atomic interaction cutoff distance [Å]\n",
        "            target_mean ([float]): Target normalisation constant\n",
        "            target_stddev ([float]): Target normalisation constant\n",
        "            normalize_atomwise (bool): Use atomwise normalisation\n",
        "            direct_force_output (bool): Compute forces directly instead of using gradient\n",
        "        \"\"\"\n",
        "        super(PaiNN, self).__init__()\n",
        "        if not target_mean:\n",
        "            target_mean = [0.0]\n",
        "        if not target_stddev:\n",
        "            target_stddev = ([1.0],)\n",
        "\n",
        "        self.num_interactions = num_interactions\n",
        "        self.hidden_state_size = hidden_state_size\n",
        "\n",
        "        num_embeddings = 119  # atomic numbers + 1\n",
        "\n",
        "        # Setup atom embeddings\n",
        "        self.atom_embeddings = nn.Embedding(num_embeddings, hidden_state_size)\n",
        "\n",
        "        # Setup message and update layers\n",
        "        self.message_layers = nn.ModuleList(\n",
        "            [\n",
        "                PaiNNMessage(hidden_state_size, cutoff=cutoff, n_rbf=n_rbf)\n",
        "                for _ in range(num_interactions)\n",
        "            ]\n",
        "        )\n",
        "        self.update_layers = nn.ModuleList(\n",
        "            [PaiNNUpdate(hidden_state_size) for _ in range(num_interactions)]\n",
        "        )\n",
        "\n",
        "        # Setup readout function\n",
        "        self.readout_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_state_size, hidden_state_size),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_state_size, 1),\n",
        "        )\n",
        "\n",
        "        # Normalisation constants\n",
        "        self.normalize_atomwise = torch.nn.Parameter(\n",
        "            torch.tensor(normalize_atomwise), requires_grad=False\n",
        "        )\n",
        "        self.normalize_stddev = torch.nn.Parameter(\n",
        "            torch.as_tensor(target_stddev), requires_grad=False\n",
        "        )\n",
        "        self.normalize_mean = torch.nn.Parameter(\n",
        "            torch.as_tensor(target_mean), requires_grad=False\n",
        "        )\n",
        "\n",
        "        # Direct force output\n",
        "        self.direct_force_output = direct_force_output\n",
        "        if self.direct_force_output:\n",
        "            self.force_readout_linear = nn.Linear(hidden_state_size, 1, bias=False)\n",
        "\n",
        "\n",
        "    def read_from_input_dict(self, input_dict, compute_forces):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dict (dict): Input dictionary of tensors with keys: nodes,\n",
        "                               nodes_xyz, num_nodes, edges, edges_displacement, cell,\n",
        "                               num_edges, targets\n",
        "            compute_forces (bool): Predict forces on atoms along with energy\n",
        "        \"\"\"\n",
        "        if compute_forces and not self.direct_force_output:\n",
        "            input_dict[\"nodes_xyz\"].requires_grad_()\n",
        "        # Unpad and concatenate edges and features into batch (0th) dimension\n",
        "        edges_displacement = unpad_and_cat(\n",
        "            input_dict[\"edges_displacement\"], input_dict[\"num_edges\"]\n",
        "        )\n",
        "        edge_offset = torch.cumsum(\n",
        "            torch.cat(\n",
        "                (\n",
        "                    torch.tensor([0], device=input_dict[\"num_nodes\"].device),\n",
        "                    input_dict[\"num_nodes\"][:-1],\n",
        "                )\n",
        "            ),\n",
        "            dim=0,\n",
        "        )\n",
        "        edge_offset = edge_offset[:, None, None]\n",
        "        edges = input_dict[\"edges\"] + edge_offset\n",
        "        edges = unpad_and_cat(edges, input_dict[\"num_edges\"])\n",
        "\n",
        "\n",
        "        # Unpad and concatenate all nodes into batch (0th) dimension\n",
        "        nodes_xyz = unpad_and_cat(\n",
        "            input_dict[\"nodes_xyz\"], input_dict[\"num_nodes\"]\n",
        "        )\n",
        "        nodes_scalar = unpad_and_cat(input_dict[\"nodes\"], input_dict[\"num_nodes\"])\n",
        "        nodes_scalar = self.atom_embeddings(nodes_scalar)\n",
        "        nodes_vector = torch.zeros(\n",
        "            (nodes_scalar.shape[0], 3, self.hidden_state_size),\n",
        "            dtype=nodes_scalar.dtype,\n",
        "            device=nodes_scalar.device,\n",
        "        )\n",
        "\n",
        "        # Compute edge distances\n",
        "        edges_distance, edges_diff = calc_distance(\n",
        "            nodes_xyz,\n",
        "            input_dict[\"cell\"],\n",
        "            edges,\n",
        "            edges_displacement,\n",
        "            input_dict[\"num_edges\"],\n",
        "            return_diff=True,\n",
        "        )\n",
        "        return nodes_scalar, nodes_vector, edges_diff, edges\n",
        "\n",
        "\n",
        "    def forward(self, input_dict, compute_forces=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dict (dict): Input dictionary of tensors with keys: nodes,\n",
        "                               nodes_xyz, num_nodes, edges, edges_displacement, cell,\n",
        "                               num_edges, targets\n",
        "            compute_forces (bool): Predict forces on atoms along with energy\n",
        "        Returns:\n",
        "            result_dict (dict): Result dictionary with keys:\n",
        "                                energy, forces\n",
        "                                Forces only included if requested (default).\n",
        "        \"\"\"\n",
        "\n",
        "        # Get properties from input dict\n",
        "        nodes_scalar, nodes_vector, edges_diff, edges = self.read_from_input_dict(input_dict, compute_forces)\n",
        "\n",
        "        # Apply interaction layers\n",
        "        for message_layer, update_layer in zip(self.message_layers, self.update_layers):\n",
        "            delta_s, delta_v = message_layer(nodes_scalar, nodes_vector, edges_diff, edges.T)\n",
        "            nodes_scalar, nodes_vector = delta_s + nodes_scalar, delta_v + nodes_vector\n",
        "\n",
        "            delta_s, delta_v = update_layer(nodes_scalar, nodes_vector)\n",
        "            nodes_scalar, nodes_vector = delta_s + nodes_scalar, delta_v + nodes_vector\n",
        "\n",
        "        # Apply readout function\n",
        "        nodes_scalar = self.readout_mlp(nodes_scalar)\n",
        "\n",
        "        # Obtain graph level output\n",
        "        graph_output = sum_splits(nodes_scalar, input_dict[\"num_nodes\"])\n",
        "\n",
        "        # Apply (de-)normalization\n",
        "        normalizer = self.normalize_stddev.unsqueeze(0)\n",
        "        graph_output = graph_output * normalizer\n",
        "        mean_shift = self.normalize_mean.unsqueeze(0)\n",
        "        if self.normalize_atomwise:\n",
        "            mean_shift = mean_shift * input_dict[\"num_nodes\"].unsqueeze(1)\n",
        "        graph_output = graph_output + mean_shift\n",
        "\n",
        "        result_dict = {\"energy\": graph_output}\n",
        "\n",
        "        # Compute forces\n",
        "        if compute_forces:\n",
        "            if self.direct_force_output:\n",
        "                forces = self.force_readout_linear(nodes_vector)\n",
        "                forces = torch.squeeze(forces, 2)\n",
        "\n",
        "                forces_reshaped = pad_and_stack(\n",
        "                    torch.split(\n",
        "                        forces,\n",
        "                        list(input_dict[\"num_nodes\"].detach().cpu().numpy()),\n",
        "                        dim=0,\n",
        "                    )\n",
        "                )\n",
        "                result_dict[\"forces\"] = forces_reshaped\n",
        "            else:\n",
        "                dE_dxyz = torch.autograd.grad(\n",
        "                    graph_output,\n",
        "                    input_dict[\"nodes_xyz\"],\n",
        "                    grad_outputs=torch.ones_like(graph_output),\n",
        "                    retain_graph=True,\n",
        "                    create_graph=True,\n",
        "                )[0]\n",
        "                forces = -dE_dxyz\n",
        "                result_dict[\"forces\"] = forces\n",
        "\n",
        "        return result_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4vamH6PD3H"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sby72vFiat5G"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "20tLbTr5atSV"
      },
      "outputs": [],
      "source": [
        "class MLCalculator(Calculator):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        implemented_properties=None,\n",
        "        device=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        if not implemented_properties:\n",
        "            implemented_properties = [\"energy\", \"energy_var\", \"forces\", \"forces_var\"]\n",
        "        self.implemented_properties = implemented_properties\n",
        "        pin_memory = (device == 'cuda')\n",
        "\n",
        "        self.batch_handler = BatchHandler(pin_memory=pin_memory)\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # self.atoms_converter = atoms_converter\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        if device:\n",
        "            model.to(device)\n",
        "\n",
        "    def calculate(\n",
        "        self, atoms=None, properties=None, system_changes=None\n",
        "    ):  # pylint:disable=unused-argument\n",
        "        if isinstance(atoms, Atoms):\n",
        "            atoms = [atoms]\n",
        "\n",
        "        if not system_changes:\n",
        "            system_changes = all_changes\n",
        "\n",
        "        if not properties:\n",
        "            properties = [\"energy\", \"forces\"]\n",
        "\n",
        "        if self.calculation_required(atoms, properties):\n",
        "            super().calculate(atoms)\n",
        "            batch = self.batch_handler.get_batch(atoms)\n",
        "\n",
        "            results = self.model(batch)\n",
        "            energies = np.array(results[\"energy\"].cpu().detach().numpy().squeeze(1), dtype='float64')\n",
        "            forces = np.array(results[\"forces\"].cpu().detach().numpy(), dtype='float64')\n",
        "\n",
        "            for force, energy, atom in zip(forces, energies, atoms):\n",
        "                atom.calc.results = {\n",
        "                    \"energy\": energy.squeeze(),\n",
        "                    \"forces\": force,\n",
        "                }  # pylint:disable=attribute-defined-outside-init\n",
        "\n",
        "                if \"energy_var\" in results:\n",
        "                    atoms.calc.results[\"energy_var\"] = results[\"energy_var\"].item()\n",
        "                if \"forces_var\" in results:\n",
        "                    atoms.calc.results[\"forces_var\"] = np.array(\n",
        "                        results[\"forces_var\"].cpu().squeeze().detach().numpy()\n",
        "                    )\n",
        "\n",
        "            for atom in atoms:\n",
        "                atom.calc.atoms = atom.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WLZ0GZIhayHg"
      },
      "outputs": [],
      "source": [
        "def batch_to_device(batch, device):\n",
        "    return {k: v.to(device) for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4JDmlwQlazch"
      },
      "outputs": [],
      "source": [
        "def get_dataset(db, energy_key=\"energy\", forces_key=\"forces\"):\n",
        "    dataset = AseDbData(\n",
        "        db,\n",
        "        TransformRowToGraphXyz(\n",
        "            cutoff=5.0,\n",
        "            energy_property=energy_key,\n",
        "            forces_property=forces_key,\n",
        "        ),\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a6XPIVJra0-b"
      },
      "outputs": [],
      "source": [
        "class DummyRow():\n",
        "    def __init__(self, atoms):\n",
        "        self.atoms = atoms\n",
        "\n",
        "    def toatoms(self):\n",
        "        return self.atoms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qul_hhIHa3g9"
      },
      "outputs": [],
      "source": [
        "class CollateAtoms:\n",
        "    def __init__(self, pin_memory):\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "    def __call__(self, graphs):\n",
        "        dict_of_lists = {k: [dic[k] for dic in graphs] for k in graphs[0]}\n",
        "        if self.pin_memory:\n",
        "            def pin(x):\n",
        "                if hasattr(x, \"pin_memory\"):\n",
        "                    return x.pin_memory()\n",
        "                return x\n",
        "        else:\n",
        "            pin = lambda x: x\n",
        "\n",
        "        collated = {k: pin(pad_and_stack(dict_of_lists[k])) for k in dict_of_lists}\n",
        "        return collated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ynQXk-6Xa2MX"
      },
      "outputs": [],
      "source": [
        "class BatchHandler:\n",
        "    def __init__(self, pin_memory):\n",
        "        self.transform = TransformRowToGraphXyz()\n",
        "        self.collate_atomsdata = CollateAtoms(pin_memory)\n",
        "\n",
        "    def get_batch(self, atoms):\n",
        "        dummyrows = [DummyRow(atom) for atom in atoms]\n",
        "        graphdata = [self.transform(row) for row in dummyrows]\n",
        "        return self.collate_atomsdata(graphdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AUVE6KKMa9wL"
      },
      "outputs": [],
      "source": [
        "def pad_and_stack(tensors):\n",
        "    \"\"\"Pad list of tensors if tensors are arrays and stack if they are scalars\"\"\"\n",
        "    if tensors[0].shape:\n",
        "        return torch.nn.utils.rnn.pad_sequence(\n",
        "            tensors, batch_first=True, padding_value=0\n",
        "        )\n",
        "    return torch.stack(tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vcRDv2ukbAGZ"
      },
      "outputs": [],
      "source": [
        "class AseDbData(torch.utils.data.Dataset):\n",
        "    def __init__(self, asedb_path, transformer, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.asedb_path = asedb_path\n",
        "        self.asedb_connection = ase.db.connect(asedb_path)\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.asedb_connection)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        # Note that ASE databases are 1-indexed\n",
        "        try:\n",
        "            return self.transformer(self.asedb_connection[key + 1])\n",
        "        except KeyError:\n",
        "            raise IndexError(\"index out of range\") # pylint: disable=raise-missing-from\n",
        "\n",
        "    def slice(self, start, stop):\n",
        "        new_list = []\n",
        "        for i in range(start, stop):\n",
        "            new_list.append(self[i])\n",
        "        return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fp4M9zMebA6D"
      },
      "outputs": [],
      "source": [
        "class TransformRowToGraphXyz:\n",
        "    \"\"\"\n",
        "    Transform ASE DB row to graph while keeping the xyz positions of the vertices\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cutoff=5.0,\n",
        "        energy_property=\"energy\",\n",
        "        forces_property=\"forces\",\n",
        "        energy_reference_property=None,\n",
        "    ):\n",
        "        self.cutoff = cutoff\n",
        "        self.energy_property = energy_property\n",
        "        self.forces_property = forces_property\n",
        "        self.energy_reference_property = energy_reference_property\n",
        "\n",
        "    def __call__(self, row):\n",
        "        atoms = row.toatoms()\n",
        "\n",
        "        edges, edges_displacement = self.get_edges(atoms)\n",
        "\n",
        "        # Extract energy and forces if they exists\n",
        "        try:\n",
        "            energy = np.copy([np.squeeze(row.data[self.energy_property])])\n",
        "        except (KeyError, AttributeError):\n",
        "            energy = np.zeros(len(atoms))\n",
        "        try:\n",
        "            forces = np.copy(row.data[self.forces_property])\n",
        "        except (KeyError, AttributeError):\n",
        "            forces = np.zeros((len(atoms), 3))\n",
        "        default_type = torch.get_default_dtype()\n",
        "\n",
        "        # pylint: disable=E1102\n",
        "        graph_data = {\n",
        "            \"nodes\": torch.tensor(atoms.get_atomic_numbers()),\n",
        "            \"nodes_xyz\": torch.tensor(atoms.get_positions(), dtype=default_type),\n",
        "            \"num_nodes\": torch.tensor(len(atoms.get_atomic_numbers())),\n",
        "            \"edges\": torch.tensor(edges),\n",
        "            \"edges_displacement\": torch.tensor(edges_displacement, dtype=default_type),\n",
        "            \"cell\": torch.tensor(np.array(atoms.get_cell()), dtype=default_type),\n",
        "            \"num_edges\": torch.tensor(edges.shape[0]),\n",
        "            \"energy\": torch.tensor(energy, dtype=default_type),\n",
        "            \"forces\": torch.tensor(forces, dtype=default_type),\n",
        "        }\n",
        "\n",
        "        return graph_data\n",
        "\n",
        "    def get_edges(self, atoms):\n",
        "        # Compute distance matrix\n",
        "        pos = atoms.get_positions()\n",
        "        dist_mat = scipy.spatial.distance_matrix(pos, pos)\n",
        "\n",
        "        # Build array with edges and edge features (distances)\n",
        "        valid_indices_bool = dist_mat < self.cutoff\n",
        "        np.fill_diagonal(valid_indices_bool, False)  # Remove self-loops\n",
        "        edges = np.argwhere(valid_indices_bool)  # num_edges x 2\n",
        "        edges_displacement = np.zeros((edges.shape[0], 3))\n",
        "\n",
        "        return edges, edges_displacement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIu6-41pbS6R"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bg1jYE1pFI41"
      },
      "outputs": [],
      "source": [
        "class LossFn(torch.nn.Module):\n",
        "    def __init__(self, energy_key, forces_key, rho):\n",
        "        super().__init__()\n",
        "        self.energy_key = energy_key\n",
        "        self.forces_key = forces_key\n",
        "        self.rho = rho\n",
        "\n",
        "    def forward(self, batch, result):\n",
        "        diff_energy = batch[self.energy_key] - result[self.energy_key]\n",
        "        err_sq_energy = torch.sum(diff_energy ** 2)\n",
        "\n",
        "        diff_forces = batch[self.forces_key] - result[self.forces_key]\n",
        "        err_sq_forces = torch.sum(diff_forces ** 2) / 3\n",
        "\n",
        "        err_sq = self.rho * err_sq_energy + (1 - self.rho) * err_sq_forces\n",
        "        return err_sq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejbhNmdbbV4a"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l_CBrh12Xbz"
      },
      "source": [
        "Set dataset and hyperparams:  \n",
        "\n",
        "- **\"transition1x\"**: use transition1x train set + QM9x test set for training, transition1x val/test set for val/test.  \n",
        "- **\"QM9x\"**: use QM9x set for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C743D2lVi-sS"
      },
      "outputs": [],
      "source": [
        "data = \"transition1x\"\n",
        "models_dir = \"neuralneb/models\"\n",
        "base_model = \"neuralneb/models/painn_t1x_0.sd\"\n",
        "start_epoch = 1\n",
        "\n",
        "batch_size = 500\n",
        "max_iters = 100000  # maximum iterations per epoch\n",
        "max_epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfo0192P2qB4"
      },
      "source": [
        "Load dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GaMsuyTXwahZ"
      },
      "outputs": [],
      "source": [
        "pin_memory = DEVICE == 'cuda'\n",
        "collate_atoms = CollateAtoms(pin_memory=pin_memory)\n",
        "\n",
        "if data == \"transition1x\":\n",
        "    t1x_train_dataset = get_dataset(\"neuralneb/data/transition1x_train.db\")\n",
        "    qm9x_train_dataset = get_dataset(\"neuralneb/data/qm9x.db\")\n",
        "\n",
        "    train_dataset = torch.utils.data.ConcatDataset([t1x_train_dataset, qm9x_train_dataset])\n",
        "\n",
        "    test_dataset = get_dataset(\"neuralneb/data/transition1x_test.db\")\n",
        "    val_dataset = get_dataset(\"neuralneb/data/transition1x_val.db\")\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, collate_fn=collate_atoms, shuffle=True\n",
        "    )\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=batch_size, collate_fn=collate_atoms\n",
        "    )\n",
        "    val_dataloader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=batch_size, collate_fn=collate_atoms\n",
        "    )\n",
        "\n",
        "else:\n",
        "    dataset = get_dataset(\"neuralneb/data/qm9x.db\")\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, collate_fn=collate_atoms\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8aIQNd7GwRgn",
        "outputId": "a106bb18-047c-43d2-c2f4-89140e427f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9225673\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW70okN33VRe"
      },
      "source": [
        "Create model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMtcI9hNwR62",
        "outputId": "7d65be44-537e-4d6a-ced4-27cb223fb781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters 2313732\n",
            "Starting from neuralneb/models/painn_t1x_0.sd\n"
          ]
        }
      ],
      "source": [
        "painn = PaiNN(\n",
        "        num_interactions=3,\n",
        "        hidden_state_size=256,\n",
        "        cutoff=5,\n",
        "        n_rbf=20\n",
        "    )\n",
        "painn.to(DEVICE)\n",
        "\n",
        "total_params = sum(\n",
        "\tparam.numel() for param in painn.parameters()\n",
        ")\n",
        "print(f\"Total number of parameters {total_params}\")\n",
        "\n",
        "if base_model:\n",
        "    statedict = torch.load(base_model)\n",
        "    painn.load_state_dict(statedict)\n",
        "    print(f\"Starting from {base_model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSauE1lr3eM4"
      },
      "source": [
        "Set loss function and optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1sFSwAm3hRu"
      },
      "outputs": [],
      "source": [
        "loss_fn = LossFn(\n",
        "    energy_key=\"energy\",\n",
        "    forces_key=\"forces\",\n",
        "    rho=0.5,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(painn.parameters(), amsgrad=True, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ZJzlCf5MWr"
      },
      "source": [
        "Train model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "icQmEw2TGVy7",
        "outputId": "3a6f52d8-5edf-4ec0-ba5e-4edb52d49621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, step: 0, loss: 36.56\n",
            "Epoch: 1, step: 1000, loss: 118.42\n",
            "Epoch: 1, step: 2000, loss: 51.75\n",
            "Epoch: 1, step: 3000, loss: 40.06\n",
            "Epoch: 1, step: 4000, loss: 33.47\n",
            "Epoch: 1, step: 5000, loss: 32.17\n",
            "Epoch: 1, step: 6000, loss: 30.49\n",
            "Epoch: 1, step: 7000, loss: 32.87\n",
            "Epoch: 1, step: 8000, loss: 43.84\n",
            "Epoch: 1, step: 9000, loss: 25.38\n",
            "Epoch: 1, step: 10000, loss: 32.36\n",
            "Epoch: 1, step: 11000, loss: 22.92\n",
            "Epoch: 1, step: 12000, loss: 24.59\n",
            "Epoch: 1, step: 13000, loss: 34.20\n",
            "Epoch: 1, step: 14000, loss: 37.72\n",
            "Epoch: 1, step: 15000, loss: 33.74\n",
            "Epoch: 1, step: 16000, loss: 27.88\n",
            "Epoch: 1, step: 17000, loss: 25.60\n",
            "Epoch: 1, step: 18000, loss: 24.31\n",
            "Epoch: 2, step: 0, loss: 27.34\n",
            "Epoch: 2, step: 1000, loss: 24.92\n",
            "Epoch: 2, step: 2000, loss: 28.86\n",
            "Epoch: 2, step: 3000, loss: 24.08\n",
            "Epoch: 2, step: 4000, loss: 26.27\n",
            "Epoch: 2, step: 5000, loss: 29.61\n",
            "Epoch: 2, step: 6000, loss: 22.22\n",
            "Epoch: 2, step: 7000, loss: 25.25\n",
            "Epoch: 2, step: 8000, loss: 25.19\n",
            "Epoch: 2, step: 9000, loss: 23.51\n",
            "Epoch: 2, step: 10000, loss: 21.57\n",
            "Epoch: 2, step: 11000, loss: 21.37\n",
            "Epoch: 2, step: 12000, loss: 27.30\n",
            "Epoch: 2, step: 13000, loss: 21.73\n",
            "Epoch: 2, step: 14000, loss: 22.08\n",
            "Epoch: 2, step: 15000, loss: 19.78\n",
            "Epoch: 2, step: 16000, loss: 29.30\n",
            "Epoch: 2, step: 17000, loss: 22.75\n",
            "Epoch: 2, step: 18000, loss: 22.16\n",
            "Epoch: 3, step: 0, loss: 21.02\n",
            "Epoch: 3, step: 1000, loss: 22.91\n",
            "Epoch: 3, step: 2000, loss: 21.75\n",
            "Epoch: 3, step: 3000, loss: 20.46\n",
            "Epoch: 3, step: 4000, loss: 21.17\n",
            "Epoch: 3, step: 5000, loss: 23.00\n",
            "Epoch: 3, step: 6000, loss: 18.12\n",
            "Epoch: 3, step: 7000, loss: 24.63\n",
            "Epoch: 3, step: 8000, loss: 20.68\n",
            "Epoch: 3, step: 9000, loss: 18.48\n",
            "Epoch: 3, step: 10000, loss: 20.87\n",
            "Epoch: 3, step: 11000, loss: 21.86\n",
            "Epoch: 3, step: 12000, loss: 21.16\n",
            "Epoch: 3, step: 13000, loss: 17.61\n",
            "Epoch: 3, step: 14000, loss: 20.36\n",
            "Epoch: 3, step: 15000, loss: 20.22\n",
            "Epoch: 3, step: 16000, loss: 52.14\n",
            "Epoch: 3, step: 17000, loss: 18.92\n",
            "Epoch: 3, step: 18000, loss: 21.89\n",
            "Epoch: 4, step: 0, loss: 19.54\n",
            "Epoch: 4, step: 1000, loss: 17.44\n",
            "Epoch: 4, step: 2000, loss: 18.73\n",
            "Epoch: 4, step: 3000, loss: 21.38\n",
            "Epoch: 4, step: 4000, loss: 16.67\n",
            "Epoch: 4, step: 5000, loss: 18.99\n",
            "Epoch: 4, step: 6000, loss: 17.84\n",
            "Epoch: 4, step: 7000, loss: 19.02\n",
            "Epoch: 4, step: 8000, loss: 17.92\n",
            "Epoch: 4, step: 9000, loss: 18.64\n",
            "Epoch: 4, step: 10000, loss: 17.85\n",
            "Epoch: 4, step: 11000, loss: 17.09\n",
            "Epoch: 4, step: 12000, loss: 15.52\n",
            "Epoch: 4, step: 13000, loss: 19.22\n",
            "Epoch: 4, step: 14000, loss: 22.62\n",
            "Epoch: 4, step: 15000, loss: 16.48\n",
            "Epoch: 4, step: 16000, loss: 16.85\n",
            "Epoch: 4, step: 17000, loss: 16.23\n",
            "Epoch: 4, step: 18000, loss: 14.63\n",
            "Epoch: 5, step: 0, loss: 17.97\n",
            "Epoch: 5, step: 1000, loss: 21.91\n",
            "Epoch: 5, step: 2000, loss: 19.18\n",
            "Epoch: 5, step: 3000, loss: 18.13\n",
            "Epoch: 5, step: 4000, loss: 19.00\n",
            "Epoch: 5, step: 5000, loss: 17.55\n",
            "Epoch: 5, step: 6000, loss: 15.41\n",
            "Epoch: 5, step: 7000, loss: 16.91\n",
            "Epoch: 5, step: 8000, loss: 17.31\n",
            "Epoch: 5, step: 9000, loss: 17.00\n",
            "Epoch: 5, step: 10000, loss: 13.56\n",
            "Epoch: 5, step: 11000, loss: 15.62\n",
            "Epoch: 5, step: 12000, loss: 18.67\n",
            "Epoch: 5, step: 13000, loss: 16.14\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(start_epoch, max_epochs):\n",
        "    step = 0\n",
        "    for batch in train_dataloader:\n",
        "        batch = batch_to_device(batch, DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        result = painn(batch)\n",
        "        loss = loss_fn(\n",
        "            batch=batch,\n",
        "            result=result,\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(f'Epoch: {epoch}, step: {step}, loss: {loss.item():.2f}')\n",
        "        step += 1\n",
        "\n",
        "        if step >= max_iters:\n",
        "            break\n",
        "\n",
        "    torch.save(painn.state_dict(), f\"neuralneb/models/painn_t1x_{epoch}.sd\")\n",
        "\n",
        "\n",
        "torch.save(painn.state_dict(), \"neuralneb/models/painn_t1x_final.sd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJjRMRBIcZHi"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muX_OXXi1rxm"
      },
      "source": [
        "## Evaluate models on validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4sgEB5yNVBr"
      },
      "source": [
        "For evaluation, we only compute loss on the energies (we remove the auxillary loss on force)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sl5grVbNUrP"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkRcPyh4NZFI"
      },
      "outputs": [],
      "source": [
        "class EvalLossFn(torch.nn.Module):\n",
        "    def __init__(self, energy_key):\n",
        "        super().__init__()\n",
        "        self.energy_key = energy_key\n",
        "\n",
        "    def forward(self, batch, result):\n",
        "        return torch.sum((batch[self.energy_key]- result[self.energy_key])**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyyXpm04NbAt"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        step = 0\n",
        "        for batch in dataloader:\n",
        "            batch = batch_to_device(batch, DEVICE)\n",
        "            result = model(batch, compute_forces=False)\n",
        "            loss = loss_fn(\n",
        "                batch=batch,\n",
        "                result=result,\n",
        "            )\n",
        "            step += 1\n",
        "            total_loss += loss\n",
        "            del result, loss, batch\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rubT7JQZNfLw",
        "outputId": "7c4954af-8c56-4b04-da14-90c39865ba7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating neuralneb/models/painn_t1x_0.sd\n",
            "Evaluating neuralneb/models/painn_t1x_1.sd\n",
            "Evaluating neuralneb/models/painn_t1x_2.sd\n",
            "Evaluating neuralneb/models/painn_t1x_3.sd\n",
            "Evaluating neuralneb/models/painn_t1x_4.sd\n",
            "[tensor(8246.6152, device='cuda:0'), tensor(7921.5464, device='cuda:0'), tensor(7978.2393, device='cuda:0'), tensor(5599.5132, device='cuda:0'), tensor(6234.5093, device='cuda:0')]\n"
          ]
        }
      ],
      "source": [
        "num_epochs_run = 5\n",
        "\n",
        "val_losses = []\n",
        "loss_fn = EvalLossFn(energy_key=\"energy\")\n",
        "\n",
        "for epoch in range(num_epochs_run):\n",
        "    model_name = f\"neuralneb/models/painn_t1x_{epoch}.sd\"\n",
        "    print(f\"Evaluating {model_name}\")\n",
        "    statedict = torch.load(model_name)\n",
        "    model = PaiNN(3, 256, 5)\n",
        "    model.to(DEVICE)\n",
        "    model.load_state_dict(statedict)\n",
        "\n",
        "    val_loss = evaluate_model(model, val_dataloader)\n",
        "    val_losses.append(val_loss)\n",
        "    del model\n",
        "\n",
        "print(val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abkyetp5dUuf"
      },
      "source": [
        "## NeuralNEB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L8VgK-68c8-V"
      },
      "outputs": [],
      "source": [
        "!wget -O neuralneb/test_reaction/p.xyz https://gitlab.com/matschreiner/neuralneb/-/raw/main/data/test_reaction/p.xyz\n",
        "!wget -O neuralneb/test_reaction/r.xyz https://gitlab.com/matschreiner/neuralneb/-/raw/main/data/test_reaction/r.xyz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qBExGgWvdw75"
      },
      "outputs": [],
      "source": [
        "def mep_fig(path, energy):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(path, energy, label=\"MEP\")\n",
        "    ax.grid()\n",
        "    ax.set_title(f\"Barrier height: {str(max(energy))[:5]} eV\")\n",
        "    ax.set_xlabel(\"Reaction Coordinate [AA]\")\n",
        "    ax.set_ylabel(\"Energy [eV]\")\n",
        "    ax.legend()\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kosWNw7QdY64"
      },
      "outputs": [],
      "source": [
        "def NeuralNEB(product, reactant, model, filename):\n",
        "    statedict = torch.load(model)\n",
        "    model = PaiNN(3, 256, 5)\n",
        "    model.load_state_dict(statedict)\n",
        "    model.eval()\n",
        "\n",
        "    product = read(product)\n",
        "    reactant = read(reactant)\n",
        "\n",
        "    assert str(product.symbols) == str(reactant.symbols), \"product and reactant must have same formula. Product: {product.symbols}, Reactant: {reactant.symbols}\"\n",
        "    atom_configs = [reactant.copy() for _ in range(10)] + [product]\n",
        "\n",
        "    for atom_config in atom_configs:\n",
        "        atom_config.calc = MLCalculator(model)\n",
        "\n",
        "    BFGS(atom_configs[0]).run(fmax=0.05, steps=1000)\n",
        "    BFGS(atom_configs[-1]).run(fmax=0.05, steps=1000)\n",
        "\n",
        "    neb = NEB(atom_configs)\n",
        "    neb.interpolate(method=\"idpp\")\n",
        "    relax_neb = NEBOptimizer(neb)\n",
        "    relax_neb.run()\n",
        "\n",
        "    nebtools = NEBTools(atom_configs)\n",
        "    fit = nebtools.get_fit()\n",
        "\n",
        "    energies = fit.fit_energies.tolist()\n",
        "    path = fit.fit_path.tolist()\n",
        "\n",
        "    mep_fig(path, energies)\n",
        "    plt.show()\n",
        "    write(f\"neuralneb/results/{filename}.gif\", images=atom_configs, format=\"gif\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "94T8A75sezXP"
      },
      "outputs": [],
      "source": [
        "product = \"neuralneb/test_reaction/p.xyz\"\n",
        "reactant = \"neuralneb/test_reaction/r.xyz\"\n",
        "model = \"neuralneb/models/painn_t1x_0.sd\"\n",
        "\n",
        "NeuralNEB(product, reactant, model, \"test_reaction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PySv3CP3HHWl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "adV2vwq3EkgS",
        "wzxNZPVfOBBR",
        "RUkxY4H2OFTG",
        "Zroi_W93QMDQ",
        "6vxVGIWEX5Ed",
        "0WzxJiKaX8E0",
        "r2qZuAE0NmTt",
        "JeAUEBnUE95d",
        "G4Zslx89Ogp7",
        "fg_v7LtWOllo",
        "p-dKzK-tO43B",
        "Sby72vFiat5G",
        "UIu6-41pbS6R"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}